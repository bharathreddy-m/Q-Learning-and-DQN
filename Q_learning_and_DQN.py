# -*- coding: utf-8 -*-
"""Q-learning and DQN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6Nm3yUO2x8QJw7Wk_DszdWIZ-iTcPZ1

# Q-Learning

Install OpenAI gym
"""

!pip install gym numpy matplotlib

"""Import required libraries"""

import numpy as np
import gym
import random
import matplotlib.pyplot as plt

"""Create the environment - We‚Äôll use FrozenLake-v1, where an agent moves on a grid to reach the goal without falling into holes."""

env = gym.make("FrozenLake-v1", is_slippery=False) # is_slippery = False for deterministic moves
state_space = env.observation_space.n # Total states in environment
action_space = env.action_space.n # Total possible actions

print(f"State Space: {state_space}, Action Space: {action_space}")

"""Initialize the Q-table - We create a Q-table (a lookup table) with zeros."""

Q_table = np.zeros((state_space, action_space))

"""Set Hyperparameters"""

learning_rate = 0.8 #How much we learn from new information
discout_factor = 0.95 # Future reward discount
epsilon = 1.0 #start with exploraiton (random moves)
epsilon_decay = 0.995 # reduce exploration over time
min_epsilon = 0.01 #Minimum exploration value
episodes = 2000 # total training episodes

"""What Does
ùõæ (Discount factor) Do?

If
ùõæ
=
0
Œ≥=0 ‚Üí The agent only cares about immediate rewards (short-sighted).

If
ùõæ
=
1
Œ≥=1 ‚Üí The agent considers long-term rewards (future rewards are fully counted).

Train the agent - We‚Äôll make the agent play the game 2000 times and learn from rewards.
"""

rewards = []

for episode in range(episodes):
    reset_output = env.reset()
    state = reset_output[0] if isinstance(reset_output, tuple) else reset_output  # FIXED

    total_reward = 0
    done = False

    while not done:
        # Choose action: Exploration vs. Exploitation
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()  # Random action (exploration)
        else:
            action = np.argmax(Q_table[state, :])  # Best action from Q-table

        # Take action, observe new state and reward
        step_output = env.step(action)
        new_state = step_output[0] if isinstance(step_output, tuple) else step_output  # FIXED
        reward = step_output[1]
        done = step_output[2]

        # Update Q-table using Bellman equation
        Q_table[state, action] = Q_table[state, action] + learning_rate * (reward + discout_factor * np.max(Q_table[new_state, :]) - Q_table[state, action])

        total_reward += reward
        state = new_state

    rewards.append(total_reward)

    # Decay epsilon
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

    # Print progress every 100 episodes
    if (episode + 1) % 100 == 0:
        print(f"Episode {episode + 1}: Average Reward: {np.mean(rewards[-100:])}")

"""**See the Q-table**"""

print(f"Final Q-table: {Q_table}")

import pandas as pd
actions = ["Left", "Down", "Right", "Up"]  # Gym's action order

df = pd.DataFrame(Q_table, columns=actions)
print(df)

"""Evaluate the model"""

test_episodes = 100
success = 0

for _ in range(test_episodes):
    reset_output = env.reset()
    state = reset_output[0] if isinstance(reset_output, tuple) else reset_output  # FIXED

    done = False

    while not done:
        action = np.argmax(Q_table[state, :])  # Choose best action
        step_output = env.step(action)

        new_state = step_output[0] if isinstance(step_output, tuple) else step_output  # FIXED
        reward = step_output[1]
        done = step_output[2]

        state = new_state

        if reward == 1:  # If the agent reaches the goal
            success += 1

print(f"Success Rate: {success / test_episodes * 100}%")

"""Plot the learning curves"""

plt.plot(np.convolve(rewards, np.ones(100)/100, mode='valid'))
plt.xlabel("Episodes")
plt.ylabel("Average Reward (Last 100)")
plt.title("Learning Curve of Q-learning Agent")
plt.show()

"""Why Is This Happening?

Starts Random: At the beginning, the agent moves randomly because of high epsilon (exploration).

Learns from Rewards: It updates the Q-table using the Bellman Equation.

Gets Smarter: Over time, it starts choosing better actions by looking at the Q-values.

Success Rate Increases: As epsilon decays, the agent makes fewer mistakes and reaches the goal more often.

**Code to Record Gameplay**
"""

import gym
print(gym.__version__)  # Check the version

state = env.reset()  # No unpacking!

import gym
import gym.wrappers
import numpy as np

# Create environment with video recording
env = gym.make("FrozenLake-v1", is_slippery=False, render_mode="rgb_array")
env = gym.wrappers.RecordVideo(env, "videos/", episode_trigger=lambda e: True)

# Reset environment (No unpacking needed in Gym v0.25.2)
state = env.reset()

done = False

while not done:
    action = np.argmax(Q_table[state, :])  # Pick the best action
    new_state, reward, done, info = env.step(action)  # FIXED: Only 4 values
    state = new_state

env.close()  # Save video



"""In Q-learning, the reward values are usually defined by the environment itself. For FrozenLake-v1, Gym automatically assigns the rewards as follows:

0 for every step (default reward).

1 when the agent reaches the goal (winning).

0 if the agent falls into a hole (losing).

**If You Want to Change Rewards**
If you want custom rewards, you can modify them in Gym's FrozenLake environment:
"""

env = gym.make("FrozenLake-v1", desc=None, is_slippery=False, reward_threshold=1.0)

"""Or manually override rewards after each step:"""

new_state, reward, done, info = env.step(action)

# Custom reward logic
if new_state == goal_state:
    reward = 10  # Increase reward for winning
elif new_state in hole_states:
    reward = -5  # Punish falling into a hole

"""# Deep Q-Network (DQN)

What‚Äôs New in DQN?

1Ô∏è‚É£ No Q-table ‚Üí We replace it with a Neural Network.

2Ô∏è‚É£ Handles large environments (e.g., Atari games, CartPole).

3Ô∏è‚É£ Learns from experience ‚Üí Uses Experience Replay to improve stability.

4Ô∏è‚É£ Uses a Target Network ‚Üí Helps in stable learning.

Steps to Implement DQN

‚úÖ Step 1: Install dependencies (TensorFlow/PyTorch, Gym, NumPy).

‚úÖ Step 2: Create a Neural Network to approximate Q-values.

‚úÖ Step 3: Implement Experience Replay to store and reuse past experiences.

‚úÖ Step 4: Train the agent in CartPole-v1 (a balance control task).

‚úÖ Step 5: Evaluate & visualize performance.

Install Required Libraries -
If you‚Äôre using Google Colab, run this first to install dependencies:
"""

# uninstall everything
!pip uninstall -y box2d box2d-py gym pygame

# install everything
!sudo apt-get update
!sudo apt-get install -y python3-dev python3-pip python3-setuptools python3-wheel python3-venv

!pip install gym==0.25.2

!pip install box2d-py

import gym
env = gym.make("LunarLander-v2")  # Test Box2D environment
env.reset()
print("Gym Box2D environment is working!")

pip install gymnasium[classic_control] torch numpy matplotlib

"""Set Up the Environment - We‚Äôll use CartPole-v1, a simple reinforcement learning environment where an agent balances a pole on a cart."""

# import libraries
import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
import matplotlib.pyplot as plt

# create environment
env = gym.make("CartPole-v1")
state_dim = env.observation_space.shape[0] # Number of state features
action_dim = env.action_space.n # Number of actions

print(f"State space: {state_dim}, Action space: {action_dim}")

"""* The state space contains 4 values (cart position, velocity, pole angle, pole velocity).

* The action space has 2 possible actions: move left (0) or move right (1).

Define the Deep Q-Network (DQN) - We need a neural network to approximate the Q-values. This network will take the state as input and output Q-values for each possible action.
"""

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)  # First hidden layer
        self.fc2 = nn.Linear(128, 128)  # Second hidden layer
        self.fc3 = nn.Linear(128, action_dim)  # Output layer

    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)  # No activation (raw Q-values)

# Initialize model
q_network = DQN(state_dim, action_dim)
target_network = DQN(state_dim, action_dim)  # Target network
target_network.load_state_dict(q_network.state_dict())  # Copy initial weights
target_network.eval()  # Target network is not trained directly

# Optimizer & Loss Function
optimizer = optim.Adam(q_network.parameters(), lr=0.001)
loss_fn = nn.MSELoss()

"""Implement Experience Replay - Experience Replay helps stabilize training by storing past experiences and sampling mini-batches for training."""

# Setting Up the Replay Buffer
import random
from collections import deque

# Define the experience replay buffer
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def add(self, experience):
        self.buffer.append(experience)

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def size(self):
        return len(self.buffer)

# Initialize replay buffer
replay_buffer = ReplayBuffer(capacity=10000)

"""Explanation

1. Uses a deque (double-ended queue) with a max size (capacity=10,000).

2. Stores (state, action, reward, next_state, done) tuples.

3. When training, samples a random mini-batch for learning.

Training Process

1. Initialize: Create the Q-network and target network.

2. Experience Collection: Play episodes, storing experiences in the replay buffer.

3. Batch Training: Randomly sample a batch from the replay buffer and compute the target Q-values.

4. Optimize the Q-network: Use gradient descent to minimize the loss.

5. Update Target Network: Every few steps, copy weights from the main Q-network to the target network.

6. Repeat until the agent learns to land the lunar module successfully!

Training Loop - Now, we‚Äôll implement the training loop where the agent interacts with the environment, stores experiences, and updates the Q-network.
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import gym
import matplotlib.pyplot as plt

# Hyperparameters
GAMMA = 0.99  # Discount factor for future rewards
LEARNING_RATE = 0.001  # Learning rate for the optimizer
EPSILON_START = 1.0  # Initial exploration rate
EPSILON_END = 0.01  # Minimum exploration rate
EPSILON_DECAY = 0.995  # Rate of epsilon decay per episode
BATCH_SIZE = 64  # Mini-batch size for experience replay
MEMORY_CAPACITY = 10000  # Capacity of the replay buffer
TARGET_UPDATE_FREQ = 10  # Update target network every X episodes
NUM_EPISODES = 500  # Number of episodes for training

# Training Loop
# Training Loop
reward_history = []
epsilon = EPSILON_START  # Start with full exploration

for episode in range(NUM_EPISODES):
    state, _ = env.reset()
    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Convert state to tensor
    episode_reward = 0
    done = False

    while not done:
        # Select action (Œµ-greedy policy)
        if random.random() < epsilon:
            action = env.action_space.sample()  # Explore
        else:
            with torch.no_grad():
                action = torch.argmax(q_network(state)).item()  # Exploit

        # Take action in environment
        next_state, reward, done, _, _ = env.step(action)
        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)

        # Store experience in replay buffer
        replay_buffer.add((state, action, reward, next_state, done))
        state = next_state
        episode_reward += reward

        # Train if buffer is sufficiently full
        if replay_buffer.size() > BATCH_SIZE:
            batch = replay_buffer.sample(BATCH_SIZE)
            states, actions, rewards, next_states, dones = zip(*batch)

            states = torch.cat(states)
            actions = torch.tensor(actions).unsqueeze(1)
            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)
            next_states = torch.cat(next_states)
            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)

            # Compute Q-values
            q_values = q_network(states).gather(1, actions)

            # Compute Target Q-values
            with torch.no_grad():
                max_next_q_values = target_network(next_states).max(1, keepdim=True)[0]
                target_q_values = rewards + (GAMMA * max_next_q_values * (1 - dones))

            # Compute loss & update network
            loss = loss_fn(q_values, target_q_values)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # Update target network every X episodes
    if episode % TARGET_UPDATE_FREQ == 0:
        target_network.load_state_dict(q_network.state_dict())

    # Decay Epsilon
    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)

    # Store reward history
    reward_history.append(episode_reward)
    print(f"Episode {episode}, Reward: {episode_reward}, Epsilon: {epsilon:.3f}")

# Save the trained model
torch.save(q_network.state_dict(), "dqn_cartpole.pth")

# visualize the performence
# Plot rewards over episodes
plt.plot(reward_history)
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.title("DQN Training Performance")
plt.show()

# Visualize the Trained Agent
import time

# Load trained model
q_network.load_state_dict(torch.load("dqn_cartpole.pth"))
q_network.eval()

state, _ = env.reset()
state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

done = False
while not done:
    with torch.no_grad():
        action = torch.argmax(q_network(state)).item()

    next_state, _, done, _, _ = env.step(action)
    state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)

    env.render()
    time.sleep(0.05)  # Slow down for visualization

env.close()

"""Testing process"""

# Load trained model
q_network.load_state_dict(torch.load("dqn_cartpole.pth"))
q_network.eval()  # Set to evaluation mode

# Run the Agent in the Environment
import time

state, _ = env.reset()
state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

done = False
total_reward = 0

while not done:
    with torch.no_grad():
        action = torch.argmax(q_network(state)).item()  # Select best action

    next_state, reward, done, _, _ = env.step(action)
    state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)

    total_reward += reward
    env.render()  # Visualize the environment
    time.sleep(0.05)  # Slow down for better visualization

env.close()
print(f"Total reward achieved: {total_reward}")

MAX_STEPS = 500  # Limit number of steps in a single episode

state, _ = env.reset()
state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

done = False
total_reward = 0
step_count = 0  # Track steps

while not done and step_count < MAX_STEPS:
    with torch.no_grad():
        action = torch.argmax(q_network(state)).item()  # Select best action

    next_state, reward, done, _, _ = env.step(action)
    state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)

    total_reward += reward
    env.render()  # Visualize the environment
    time.sleep(0.01)  # Reduce delay for faster visualization

    step_count += 1  # Increment step count

env.close()
print(f"Total reward achieved: {total_reward}")

"""To record the video"""

pip install imageio imageio[ffmpeg]

import gym
import torch
import imageio
import numpy as np

# Load trained model
q_network.load_state_dict(torch.load("dqn_cartpole.pth", weights_only=True))  # Use weights_only=True
q_network.eval()

# Create an environment wrapped with a video recorder
video_dir = "./videos"  # Directory to store the video
env = gym.make("CartPole-v1", render_mode="rgb_array")
env = gym.wrappers.RecordVideo(env, video_dir, episode_trigger=lambda x: True)

# Reset environment safely
reset_output = env.reset()
state = reset_output[0] if isinstance(reset_output, tuple) else reset_output  # Handle tuple output

done = False

while not done:
    state = np.array(state, dtype=np.float32)  # Ensure correct dtype
    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Convert to tensor

    with torch.no_grad():
        action = q_network(state_tensor).argmax(dim=1).item()  # Choose best action

    # Handle different Gym step() return formats
    step_output = env.step(action)

    if len(step_output) == 5:  # New API (5 values)
        state, reward, terminated, truncated, info = step_output
        done = terminated or truncated
    elif len(step_output) == 4:  # Old API (4 values)
        state, reward, done, info = step_output

env.close()

print(f"üé• Video saved in {video_dir}. Check the folder for playback!")

import gym
from gym.wrappers import RecordVideo
import torch

env = gym.make("CartPole-v1", render_mode="rgb_array")
env = RecordVideo(env, video_folder="./videos", episode_trigger=lambda e: True)

# Handle different Gym versions for reset()
reset_output = env.reset()
if isinstance(reset_output, tuple):
    state, info = reset_output  # New API
else:
    state = reset_output  # Old API

done = False

while not done:
    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
    with torch.no_grad():
        action = q_network(state_tensor).argmax().item()  # Choose best action

    # Handle different Gym versions for step()
    step_output = env.step(action)
    if len(step_output) == 5:
        state, reward, done, truncated, info = step_output  # New API
    else:
        state, reward, done, info = step_output  # Old API

env.close()

from IPython.display import Video

Video("./videos/rl-video-episode-0.mp4")  # Play the video

